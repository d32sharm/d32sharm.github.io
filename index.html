<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 
  <title>Dhruv Sharma - Robotics and Learning</title>
  
  <meta name="author" content="Christopher Agia">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="main.css">
  <link rel="icon" type="image/png" href="images/ml_icon.png">

  <style>
  .accordion {
    background-color: #eee;
    color: #444;
    cursor: pointer;
    padding: 18px;
    width: 100%;
    border: none;
    text-align: right;
    outline: none;
    font-size: 22px;
    font-family: monospace;
    transition: 0.4s;
  }

  .active, .accordion:hover {
    background-color: #ccc;
  }

  .panel {
    padding: 0 0px;
    width:100%;
    vertical-align:middle;
    background-color: #f9f9f9;
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.4s ease-out;
  }

  </style>  

  <style>
    
    /* Style the button that is used to open and close the collapsible content */
    .collapsible {
      background-color: #f9f9f9;
      /* color: #444; */
      cursor: pointer;
      /* padding: 18px; */
      width: 100%;
      border: none;
      text-align: right;
      outline: none;
      /* font-size: 15px; */
      font-family: monospace;
      transition: 0.4s;
    }

    /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
    .active, .collapsible:hover {
      background-color: #ccc;
    }

    /* Style the collapsible content. Note: hidden by default */
    .content {
      /* padding: 0 18px; */
      display: none;
      overflow: hidden;
      background-color: #f9f9f9;
      transition: max-height 0.4s ease-out;
    }

  </style>
  
</head>



<body>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/dhruvsharma-pic-circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dhruvsharma-pic-circle.png" class="hoverZoomLink"></a>
            </td>

            <td style="padding:2.5%;width:63%;vertical-align:middle;background-color: #f9f9f9">
              <p style="text-align:center">
                <name>Dhruv Sharma</name>
              </p>
              <p>
                I'm an MSc graduate from the Department of Computer Science at the University of Toronto where I was a part of  
                <a href="https://rvl.cs.toronto.edu/">Robot Vision and Learning Laboratory</a>. I was advised 
                by <a href="http://www.cs.toronto.edu/~florian/"><strong>Florian Shkurti</strong></a> at 
                <a href="http://iprl.stanford.edu/">RVL</a>. Prior to UofT, I have worked at NVIDIA as a Software Engineer in 
                their autonomous driving team at New Jersey reporting to Urs Muller and Beat Flepp. I am interested in general in autonomous
                robotics and specifically in perception for autonomous robots.
              </p>
              <!-- <p>
                My research seeks to extend the capabilities of autonomous robots in long horizon .
              </p> -->
             <!--  <p>
                I set aside a few hours each week for research discussions and mentorship. If you feel this could be of 
                benefit to you, please book an open slot <a href="https://calendly.com/agiachris/virtual-research-chat">here</a>.
              </p> -->
              <button type="button" class="collapsible">..more</button>
              <div class="content">
                <p>
                  During my time at Waterloo, I have worked at numerous different companies such as NVIDIA, Capital One, Diebold Nixdorf in 
                  capacity of Software Engineer, Data Scientist, Deep Learning Intern, etc. 

                  <!-- I graduated from the 
                  <a href="https://www.sgs.utoronto.ca/programs/computer-science/">Computer Science Robotics</a> 
                  program at the University of Toronto, where I was advised by <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a> 
                  at <a href="https://rvl.cs.toronto.edu/">RVL</a> and the <a href="https://vectorinstitute.ai/">Vector Institute</a>. 
                  I've been fortunate to collaborate with <a href="https://liampaull.ca/">Liam Paull</a> at 
                  <a href="https://montrealrobotics.ca/">MILA</a>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a> 
                  and <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a> at <a href="https://www.cim.mcgill.ca/~mrl/">McGill</a>, 
                  <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Goldie Nejat</a> at the 
                  <a href="http://asblab.mie.utoronto.ca/">University of Toronto</a>, and colleagues from 
                  <a href="https://www.microsoft.com/en-us/research/group/reinforcement-learning-redmond/">Microsoft Research</a> and 
                  <a href="https://ai.facebook.com/">Facebook AI Research</a>. 
                  In industry, I had the opportunity to work on multi-agent reinforcement learning in mixed reality environments at 
                  <a href="https://www.microsoft.com/en-us/research/project/robotics-and-mixed-reality/">Microsoft</a>, perception 
                  and localization for self-driving vehicles at <a href="http://www.noahlab.com.hk/#/home">Huawei Noah's Ark Lab</a> with
                  <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>, and 
                  language-agnostic ABI simulators at <a href="https://about.google/">Google</a> Cloud. -->
                </p>
              </div>

              <p style="text-align:center">
                dhruv[at]cs.toronto.edu &nbsp/&nbsp
                <!-- <a href="data/CA_Resume_2022.pdf">Resume</a> &nbsp/&nbsp -->
                <a href="data/CA_ResearchCV_2022.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <!-- <a href="data/ChrisAgiaBio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/dhruvsharmauw/" target="_blank"> LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/d32sharm" target="_blank"> GitHub</a>
              </p>

            </td>

          </tr>
        </tbody></table>
        


        <p></p>
<!-- ------------------------------------------------------------------------------------------------------------------------------ -->



<!--         <button class="accordion">..Education</button>
        <div class="panel">
        <p></p> -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="text-align: right; background-color: #eee">
              <heading style="font-family: monospace; font-size: 22px; color: #444">..Education</heading>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UofT-Crest-Square.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>M.S.c in Computer Science</papertitle>
              <br>
                Department of Computer Science, University of Toronto
              <br>
                Sep 2019 - Jan 2021 | Toronto, ON
              <p> </p> 
              <em><strong>Ontario Graduate Scholarship</strong></em>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UofW-Logo.jpeg" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>B.A.Sc in Mechatronics Engineering</papertitle>
              <br>
                Faculty of Applied Science and Engineering, University of Waterloo 
              <br>
                Sep 2013 - May 2018 | Waterloo, ON
              <p> </p> 
              <em><strong>President's Research Award</strong></em>
              <br>
              <em><strong>NSERC Undergraduate Research Award</strong></em>
              <br>
              <em><strong>Dean's Honour List - 2015-2018</strong></em>
            </td>
          </tr>
        </tbody></table>
        </div>



        <p></p>
<!-- ------------------------------------------------------------------------------------------------------------------------------ -->



        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested bridging concepts from <strong>Robotics</strong>, <strong>Deep Learning</strong>, and <strong>Computer Vision</strong> to build improved task planning, motion planning, decision-making and control systems. More recently, I've explored the use of <strong>unsupervised representation learning</strong> and <strong>reinforcement learning</strong> to create observational / world models that faciliate optimal planning and control. 
              </p>
              <p>
                I've also lead and contributed to perception projects related to: 3D Scene Understanding, 2D/3D Semantic Scene Completion, 2D/3D Object Detection, LiDAR segmentation, and more!
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="text-align: right; background-color: #eee">
              <heading style="font-family: monospace; font-size: 22px; color: #444">..Research</heading>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                I'm interested bridging concepts from <strong>Robotics</strong>, <strong>Deep Learning</strong>, and <strong>Computer Vision</strong> 
                to build improved task & motion planning, decision-making and control systems. I've recently explored modern <strong>learning-based planners</strong> 
                and their amenability to <strong>long-horizon robotic tasks</strong> in large-scale <strong>3D scene graphs</strong> - 
                details in <a href="https://drive.google.com/file/d/1LjTdgwuiJa-gIiVbbqj9vh-qoEZgqkb_/view?usp=sharing">BASc thesis</a>.
                Before that, I researched couplings in <strong>representation learning</strong> and <strong>reinforcement learning</strong> to create 
                observational models that facilitate improved control.
              </p>
              <p>
                I've also lead and contributed to projects related to: semantic localization, 3D scene understanding, 3D semantic scene completion, 
                2D/3D object detection, LiDAR segmentation, and more!
              </p>
            </td>
          </tr>
        </tbody></table>
 -->
       <!--  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Journal Papers</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sem_loc.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Lightweight Semantic-aided Localization with Spinning LiDAR Sensor</papertitle>
              <br>
              Yuan Ren*, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>, <a href="https://rancheng.github.io/about/">Ran Cheng</a>, <strong>Christopher Agia</strong>
              <br>
              [Patented]. <em>IEEE Transactions on Intelligent Vehicles (T-IV)</em>, 2021
              <br>
              <a href="papers/SemanticLoc-TIV-2021.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9495210">IEEExplore</a>
              <p>How can semantic information be leveraged to improve localization accuracy in changing environments? We present a robust LiDAR-based localization 
                algorithm that exploits both semantic and geometric properties of the scene with an adaptive fusion strategy.</p>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="140" src="images/sim2real_drl_rough_nav.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Sim-to-Real Pipeline for Deep Reinforcement Learning for Autonomous Robot Navigation in Cluttered Rough Terrain</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/richardhu-12dsf/?originalSubdomain=ca">Han Hu*</a>, <a href="https://www.linkedin.com/in/kczhang/?originalSubdomain=ca">Kaicheng Zhang*</a>, <a href="https://www.linkedin.com/in/aaron-hao-tan/?originalSubdomain=ca">Aaron Hao Tan</a>, <a href="https://www.linkedin.com/in/michael-ruan-0822/">Michael Ruan</a>, <strong>Christopher Agia</strong>, <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/"> Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em> at <em>IROS</em>, 2021 | Prague, CZ
              <br>
              <a href="papers/Sim2RealDRLNav-RAL-2021.pdf">PDF</a> / <a href="https://www.youtube.com/watch?v=dtYlNWvK-7k&ab_channel=AutonomousSystemsandBiomechatronicsLab%28UniversityofToronto%29">Video</a> / <a href="https://ieeexplore.ieee.org/document/9468918">IEEExplore</a>
              <p>Deep Reinforcement Learning is effective for learning robot navigation policies in rough terrain and cluttered simulated environments. 
                In this work, we introduce a series of techniques that are applied in the policy learning phase to enhance transferability to real-world domains.</p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Conference Papers</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eqpaper.jpeg" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Augmenting Imitation Experience via Equivariant Representations</papertitle>
              <br>
              <strong>Dhruv Sharma*</strong>, <a href="https://www.alihkw.com/">Alihusein Kuwajerwala*</a>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022 | Philadelphia, USA
              <br>
	            <a href="https://arxiv.org/pdf/2110.07668.pdf" target="_blank">PDF</a> / <a href="">Poster</a> / <a href="">Project Site</a>
              <p>Behavriour cloning comes with the covariate shift problem, i.e. an autonomous agent fails if the data seen at the test time is 
              different from that seen at training time. This is usually addressed using additional sensors to collect data to train the agents.
             This work proposes a method to train robust behaviour cloning policies using the concept of image equivariance.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/elppaper.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Facial Recognition with Encoded Local Projections</papertitle>
              <br>
              <strong>Dhruv Sharma</strong>, Sarim Zafar, Morteza Babaie, <a href="https://uwaterloo.ca/systems-design-engineering/profile/tizhoosh">H.R. Tizhoosh</a>
              <br>
              <em>IEEE-Symposium Series on Computational Intelligence</em>, 2018 | Bengaluru, India
              <br>
              <a href="https://arxiv.org/pdf/1809.06218.pdf">PDF</a>
              <p>This paper attempts for the first time to utilize
                ELP descriptor as primary features for facial recognition
                and compare the results with LBP histogram on the
                Labeled Faces in the Wild dataset. We have evaluated
                descriptors by comparing the chi-squared distance of each
                image descriptor versus all others as well as training
                Support Vector Machines (SVM) with each feature vector.
                In both cases, the results of ELP were better than LBP in
                the same sub-image configuration</p>
            </td>
          </tr>
        </tbody></table>

       

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Theses</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/theses.jpeg" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Augmenting Imitation Experience via Equivariant Representations</papertitle>
              <br>
              <strong>Dhruv Sharma*</strong>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
              <br>
              <em>Department of Computer Science, University of Toronto</em>, 2021 | Toronto, CA
              <br>
              <a href="https://drive.google.com/file/d/14dRhhc9rOCT1i0CHzwEaU7fG1PfX1ETL/view?usp=sharing">PDF</a>
              <p>My MSc. graduate work examines the problem of covariate shift in behavriour cloning. Covariate Shift is when an autonomous agent fails if 
                the data seen at the test time is different from that seen at training time. This is usually addressed using additional sensors to collect data to train the agents.
             This work proposes a method to train robust behaviour cloning policies using the concept of image equivariance.</p>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patents</heading>
              <p>Several components of my industry research projects were patented alongside submitting to conference / journal venues.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/patent_road_segmentation.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Systems and Methods for Generating a Road Surface Semantic Segmentation Map from a Sequence of Point Clouds</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="https://rancheng.github.io/about/">Ran Cheng</a>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              Application No. 17/676,131. U.S. Patent and Trademark Office, 2022
              <p>Relates to processing point clouds for autonomous driving of a vehicle. More specifically, relates to processing a sequence of point 
                clouds to generate a birds-eye-view (BEV) image of an environment of the vehicle which includes pixels associated with road surface labels.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/patent_scene_completion.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Methods and Systems for Semantic Scene Completion for Sparse 3D Data</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng</a>, <strong>Christopher Agia</strong>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              Application No. 17/492,261. U.S. Patent and Trademark Office, 2022
              <p>Relates to methods and systems for generating semantically completed 3D data from sparse 3D data such as point clouds.</p>
            </td>
          </tr>
        </tbody></table>
 -->


<!-- ------------------------------------------------------------------------------------------------------------------------------ -->



        <button class="accordion">..Work Experience</button>
        <div class="panel">
        <p></p>
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UofT-Crest-Square.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Graduate Researcher</papertitle>
              <br>
                <a href="https://rvl.cs.toronto.edu/">Robot Vision and Learning Lab</a>, University of Toronto
              <br>
                Sept 2019 - <strong>March 2021</strong> | Toronto, Ontario
              <p> 
                Research & development at the intersection of robotics, AI, and computer vision. Developed techniques to enhance visual navigation using imitation learning 
              while using less data. Used concept of image equivariance to improve visual navigation policies.</p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nvidia_logo.jpeg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineer - Autonomous Driving</papertitle>
              <br>
                <a href="https://www.nvidia.com/">NVIDIA</a> | Advised by Dr. Urs Muller and Dr. Beat Flepp
              </br>
              <br>
                Oct 2020 - <strong>Oct 2021</strong> | Holmdel, NJ
              </br>
              <p>
                Worked on the NVIDIAs research group developing end to end deep networks for <a href="https://www.youtube.com/watch?v=-96BEoXJMs0" target="_blank">NVIDIAs AI Car</a>. 
                Developed the self driving simulator as well as additional infrastructure to train and test networks.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UofW-Logo.jpeg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Research Engineer</papertitle>
              <br>
                <a href="https://www.autonomoose.net/">Waterloo Self Driving Car Project</a>, University of Waterloo
              <br>
                July 2018 - Sept 2018 | Waterloo, Ontario
              <p> 
                Performed simulation based research in autonomous driving using <a href="https://www.coppeliarobotics.com/" target="_blank">Copelia Robotics V-rep simulator</a> and <a href="https://www.unrealengine.com/en-US" target="_blank">Unreal Engine</a> based 
                simulator. Integrated the dynamic vehicle model of the vehicle in the simulation pipeline.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nvidia_logo.jpeg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Deep Learning Intern</papertitle>
              <br>
                <a href="https://www.nvidia.com/">NVIDIA</a> | Multiple Internships
                </br> 
                Jan 2016 - Sep 2017 | Santa Clara & New Jersey Research Lab
              <p> 
               Worked on developing autonomous driving technology on <a href="https://www.nvidia.com/en-us/self-driving-cars/" target="_blank">NVIDIA Drive hardware</a>. 
               Contributed to deep learning and robotics pipelines. Trained and tested on road deep neural netwrks to run the car. Contributed to <a href="https://www.youtube.com/watch?v=mCmO_5ZxdvE" target="_blank">demo</a> video shoots.
              </p> 
            </td>
          </tr>
        </tbody></table>
       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/capitalonelogo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Data Scientist Intern</papertitle>
              <br>
                Capital One, <a href="https://www.thecapitalonelab.com/about-us">Capital One KW Lab</a>
              <br>
                May 2015 - Aug 2015 | Kitchener, ON
              <p> 
                Created an NLP pipeline to classify and analyze customer feedback. Helped improve company's net promoter score KPI. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dieboldnixdorf_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Developer Intern</papertitle>
              <br>
                <a href="https://www.dieboldnixdorf.com/">Diebold Nixdorf</a>, Prev. Phoenix Interactive Inc. 
              <br>
                Sept 2014 - Dec 2014 | London, ON
              <p> 
              	Software development for Phoenix's flagship VisaATM terminal software. Developed new functionalities and implemented testing using Google frameworks.
              </p> 
            </td>
          </tr>
        </tbody></table>

       <!--  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UofT-Crest-Square.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics Research Intern</papertitle>
              <br>
                <a href="http://asblab.mie.utoronto.ca/">Autonomous Systems and Biomechatronics Lab</a> | Advised by <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Prof. Goldie Nejat</a>
              <br>
                <a href="https://www.mie.utoronto.ca/">Department of Mechanical and Industrial Engineering</a>, University of Toronto
              <br>
                May 2018 - Aug 2018 | Toronto, ON
              <p> 
              	Search and rescue robotics - research on the topics of Deep Reinforcement Learning and Transfer Learning for autonomous robot navigation in rough and 
                hazardous terrain. ROS (Robot Operating System) software development for various mobile robots.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ge_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                General Electric, <a href="https://www.gegridsolutions.com/index.htm">Grid Solutions</a>
              <br>
                May 2017 - Aug 2017 | Markham, ON
              <p> 
              	Created customer-end software tools used to accelerate the transition/setup process of new protection and control systems upon upgrade. 
                Designed the current Install-Base and Firmware Revision History databases used by GE internal service teams.
              </p> 
            </td>
          </tr>
        </tbody></table>
        -->

        </div>


        <p></p>
<!-- ------------------------------------------------------------------------------------------------------------------------------ -->



        <button class="accordion">..Projects and Competitions</button>
        <div class="panel">
          <p></p>
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects and Competitions</heading>
              <p>
                <strong>Learn by doing</strong> - I've had the opportunity to work on many interesting projects that range across industries such as Robotics, Health Care, Finance, Transportation, and Logistics.  
              <p>
              	Links to the source code are embedded in the project titles.
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                <strong>Coming soon!</strong> 
              </p>
                
            <!--   <p>
                Links to the source code are embedded in the project titles.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        
        </div>

      </td>
    </tr>
  </table>


  <script>
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
      acc[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var panel = this.nextElementSibling;
        if (panel.style.maxHeight) {
          panel.style.maxHeight = null;
        } else {
          panel.style.maxHeight = panel.scrollHeight + "px";
        } 
      });
    }
  </script>


  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }
  </script>



  <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles-js"></div>
  <script>
    particlesJS.load("particles-js", "particles_config/particlesjs-config-gray.json",
    function(){
        console.log("particles.json loaded...")
    })
  </script>


</body>
</html>
